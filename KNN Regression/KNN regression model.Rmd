---
title: "DA.5020.P3.Xiaoluan.Huang"
author: "Xiaoluan Huang"
date: "2022-08-13"
output: html_document
---
```{r}
#load packages 
library(tidyverse)
library(moments)
library(corrplot)
library(lubridate)
library(psych)
```

Question 1 — (20 points) +10 optional points 

CRISP-DM: Data Understanding 

•  Load the NYC Green Taxi Trip Records data into a data frame or tibble. 
```{r}
tripdata_df<-read.csv("2018_Green_Taxi_Trip_Data.csv",  header = TRUE, sep= ",", stringsAsFactors = FALSE)
```

•  Data exploration: explore the data to identify any patterns and analyze the relationships between the 
features and the target variable i.e. tip amount. At a minimum, you should analyze: 1) the distribution, 
2) the correlations 3) missing values and 4) outliers — provide supporting visualizations and explain 
all your steps. 
```{r}
glimpse(tripdata_df)
summary(tripdata_df)
str(tripdata_df)
dim(tripdata_df)
#checks each column for the number of NA values. "ehail_fee" column has 1048575 NA values
colSums(is.na(tripdata_df))
```


```{r}
#convert "lpep_pickup_datetime" and "lpep_dropoff_datetime" to POSIXlt data type
tripdata_df$lpep_pickup_datetime<- as.POSIXct(tripdata_df$lpep_pickup_datetime, format="%m/%d/%Y %H:%M")
tripdata_df$lpep_dropoff_datetime<- as.POSIXct(tripdata_df$lpep_dropoff_datetime, format="%m/%d/%Y %H:%M")
#convert remaining columns to numeric data type
tripdata_df$VendorID<-as.numeric(tripdata_df$VendorID)
tripdata_df$RatecodeID<-as.numeric(tripdata_df$RatecodeID)
tripdata_df$PULocationID<-as.numeric(tripdata_df$PULocationID)
tripdata_df$DOLocationID<-as.numeric(tripdata_df$DOLocationID)
tripdata_df$passenger_count<-as.numeric(tripdata_df$passenger_count)
tripdata_df$fare_amount<-as.numeric(tripdata_df$fare_amount)
tripdata_df$total_amount<-as.numeric(tripdata_df$total_amount)
tripdata_df$payment_type<-as.numeric(tripdata_df$payment_type)
tripdata_df$trip_type<-as.numeric(tripdata_df$trip_type)
tripdata_df$store_and_fwd_flag<-as.numeric(as.factor(tripdata_df$store_and_fwd_flag))
```


```{r}
str(tripdata_df)
summary(tripdata_df)
```

```{r}
#"RatecodeID" should only be between 1 and 6, but the max is 99.
#"fare_amount", "extra", "mta_tax", "tip_amount", "improvement_surcharge", and "total_amount" all have negative values which could be error or refunds and should be excluded. 
#remove "ehail_fee" column since it is entirely made up of NAs. And remove NAs from "trip_type", "fare_amount", "total_amount". 
#And filtering out data in the year of 2018 only. 
tripdata<-tripdata_df%>%
  select(-ehail_fee) %>%
  drop_na(trip_type, fare_amount, total_amount) %>%
  filter(RatecodeID<7, fare_amount>=0, extra>=0,mta_tax>=0, tip_amount>=0,improvement_surcharge>=0,
        total_amount>=0) %>%
  filter(year(lpep_pickup_datetime)==2018&year(lpep_pickup_datetime)==2018)

summary(tripdata)
#check if all NAs are removed
colSums(is.na(tripdata))
```


```{r}
#trip_distance distribution
skewness(tripdata$trip_distance)
ggplot(tripdata, aes(x=trip_distance))+geom_histogram()

ggplot(tripdata, aes(x=log10(trip_distance)))+geom_histogram(aes(y=..density..))+
  stat_function(fun=dnorm,args = list(mean=log10(mean(tripdata$trip_distance)),                                     
                                      sd=log10(sd(tripdata$trip_distance))),color="red", size=1)
```

  - Trip_distance is skewed to the right by 3.482477 

```{r}
skewness(tripdata$fare_amount)
ggplot(tripdata, aes(x=fare_amount))+geom_histogram()
ggplot(tripdata,aes(x=log10(fare_amount)))+geom_histogram(aes(y=..density..))+
  stat_function(fun=dnorm,args=list(mean=log10(mean(tripdata$fare_amount)),
                                    sd=log10(sd(tripdata$fare_amount))),colour='red',size=1)
```

  - Fare_amount is skewed to the right by 6.194969 

```{r}
skewness(tripdata$total_amount)
ggplot(tripdata, aes(x=total_amount))+geom_histogram()
ggplot(tripdata,aes(x=log10(total_amount)))+geom_histogram(aes(y=..density..))+
  stat_function(fun=dnorm,args=list(mean=log10(mean(tripdata$total_amount)),
                                    sd=log10(sd(tripdata$total_amount))),colour='red',size=1)
```

  - Total_amount is skewed to the right by 5.558879. 

```{r}
skewness(tripdata$tip_amount)
ggplot(tripdata, aes(x=tip_amount))+geom_histogram()
ggplot(tripdata, aes(x=log10(tip_amount)))+geom_histogram(aes(y=..density..))+stat_function(fun=dnorm,args = list(mean=log10(mean(tripdata$tip_amount)), sd=log10(sd(tripdata$tip_amount))),color="red", size=1)
```

  - Tip_amount is skewed to the right by 13.48808. 

```{r}
#correlations between independent variables 
cor(tripdata[,-c(2,3,13)])
corrplot(cor(tripdata[,-c(2,3,13)]),diag=F,type = "upper")
```

  - Strong correlations between independent variables:
    - "trip_distance" and "fare_amount" has a correlation of 0.90.
    - "RatecodeID" and "mta_tax" has a correlation of -0.94.
    - "RatecodeID" and "improvement_surcharge" has a correlation of -0.92.
    - "mta_tax" and "improvement_surcharge" has a correlation of 0.98. 
    - "fare_amount" and "total_amount" has a correlation of 0.977.
    - "trip_distance" and "total_amount" has a correlation of 0.894.
    - "RatecodeID" and "trip_type" has a correlation of 0.936.
    - "mta_tax" and "trip_type" has a correlation of -0.968.
    - "improvement_surcharge" and "trip_type" has a correlation of -0.9796. 


```{r}
#correlations between dependent variable and independent variables. 
cor(tripdata$tip_amount,tripdata[,-c(2,3)] )
```

  - By looking at the correlations between "tip_amount" and the rest of the variables: "trip_distance", "fare_amount", "tolls_amount", total_amount", "payment_type" and "DOLocationID" seems to have stronger correlation with "tip_amount". 

    
```{r}
#outliers 
zscore<-as.data.frame(sapply(tripdata[,9:18],function(z) (abs(z-mean(z))/sd(z))))

out<-function(zscore){
  result<-which(zscore>3)
  length(result)
}

apply(zscore,2,out)
```

  - The outliers are selected if their z scores are greater then 3 which means they are more than 3 sd away from the mean. There are total of 20943 outliers in trip distance, 19068 in fare amount, 291 in extra, 19619 in mta tax,14444 in tip amount, 13848 in tolls amount, 18749 in improvement surcharge, 18899 in total amount, 1490 in payment type, and 18455 in trip type.


Tip: remember that you have worked with this dataset in your previous assignments. You are free to reuse any code that support your analysis. 

•  **Feature selection**: identify the features/variables that are good indicators and should be used to predict the tip amount. Note: this step involves selecting a subset of the features that will be used to build the predictive model. If you decide to omit any features/variables ensure that you briefly state the reason. 

  - Use backwards selection: start with all variables from the dataset except "lpep_pickup_datatime" and "lpep_dropoff_datetime".
```{r}
summary(lm(tip_amount~ VendorID+ store_and_fwd_flag+RatecodeID+PULocationID+
             DOLocationID+passenger_count+trip_distance+
             fare_amount+extra+mta_tax+tolls_amount+improvement_surcharge+
             total_amount+payment_type+trip_type, data=tripdata))
```

- Backwards selection continues:
```{r}
#remove "store_and_fwd_flag" which has the highest insignificant p-value (0.267044)
summary(lm(tip_amount~  VendorID+RatecodeID+PULocationID+
             DOLocationID+passenger_count+trip_distance+
             fare_amount+extra+mta_tax+tolls_amount+improvement_surcharge+
               total_amount+payment_type+trip_type , data = tripdata))

```

  - All independent variables are significant(p-value<0.05) at this point. 
  - For a backward selection, variables we would keep are: VendorID, RatecodeID, PULocationID, DOLocationID, passenger_count, trip_distance,
fare_amount, extra, mta_tax, tolls_amount, improvement_surcharge, total_amount, payment_type, trip_type. 

- Forward selection:

```{r}
#start with intercept-only model.
lm(tip_amount~1,data = tripdata)
#define model with all predictors
lm(tip_amount ~ VendorID+ store_and_fwd_flag+RatecodeID+PULocationID+
             DOLocationID+passenger_count+trip_distance+
             fare_amount+extra+mta_tax+tolls_amount+improvement_surcharge+
             total_amount+payment_type+trip_type, data=tripdata)

#perform forward selection using step():
step(lm(tip_amount~1,data = tripdata), direction='forward',  
              scope=formula(lm(tip_amount ~ VendorID+ store_and_fwd_flag+RatecodeID+PULocationID+
             DOLocationID+passenger_count+trip_distance+
             fare_amount+extra+mta_tax+tolls_amount+improvement_surcharge+
             total_amount+payment_type+trip_type, data=tripdata)), trace=0)

```

  - Forward selection selects variables: VendorID, RatecodeID, PULocationID, DOLocationID, passenger_count, trip_distance,
fare_amount, extra, mta_tax, tolls_amount, improvement_surcharge, total_amount, payment_type, trip_type. 
  - Both forwards and backwards selection yields the same group of independent variables. 

```{r}
#subsets from the results of stepwise selection:
sub_tripdata<-tripdata %>%
  select(VendorID,RatecodeID,PULocationID,
             DOLocationID,passenger_count,trip_distance,
             fare_amount,extra, mta_tax,tolls_amount,improvement_surcharge,
             total_amount,payment_type,trip_type, tip_amount)
#sub_tripdata

```

•  **Feature engineering**: (+10 bonus points): create a new feature and analyze its effect on the target variable (e.g. the tip amount). Ensure that you calculate the correlation coefficient and also use visualizations to support your analysis. Summarize your findings and determine if the new feature is a good indicator to predict the tip amount. If it is, ensure that you include it in your model. If it is not a good indicator, explain the reason. 

  -We are creating a new variable: "speed" which is the the travel distance divided by travel time. 
```{r}
tripdat<-tripdata%>%
  mutate(hour=as.numeric(round((lpep_dropoff_datetime-lpep_pickup_datetime)/3600, 2)))%>%
  mutate(speed=trip_distance/hour)%>%
  drop_na(speed)
  tripdat<- tripdat[is.finite(tripdat$speed),]
tripdat<-tripdat%>%
  mutate(tripdat,z=abs(tripdat$speed-mean(tripdat$speed))/sd(tripdat$speed))%>%
  filter(!z>3)%>%
  arrange(desc(speed))
  #view tridat
summary(tripdat)

cor(tripdat$tip_amount,tripdat$speed)
corrplot(cor(tripdat[,c('speed','tip_amount')]),diag=F,type = "upper")
pairs.panels(tripdat[c('speed','tip_amount')])

```

  - Before doing the analysis, we cleaned the data by removing the NAs and infs (some time gap b/w pickup and drop off were too small which gave us "0"or very small number for time, and led to NA or inf when calculating speed). There are some speed data looks abnormal(e.g.speed:3000mile/hr,dis:34 miles,time:0.1hour), therefore, we removed the outliers of speed(z=3). The correlation b/w speed and tip amount is 0.1730791 which indicates very weak correlation.The correlation matrix gave the correlation very light color which also indicates a very weak correlation. The pair panel shows separated histograms of tip amount and speed(mile/hour), and a line plot of the relationship b/w the two. the graph shows the tips occurs at all speed level, and no association b/w speed and tip amount.


**Question 2 — (20 points) **

CRISP-DM: Data Preparation 

•  Prepare the data for the modeling phase and handle any issues that were identified during the exploratory data analysis. At a minimum, ensure that you: 

•  Preprocess the data: handle missing data and outliers, perform any suitable data transformation steps, etc. Also, ensure that you filter the data. The goal is to predict the tip amount, therefore you need to ensure that you extract the data that contains this information. Hint: read the data dictionary. 
  
  - In the previous question:
  - We have removed missing values and identified outliers, and converted the data type of the columns as necessary. 
  - We filtered the dataset to only data within the year of 2018, and removed all negative values from "fare_amount", "extra", "mta_tax", "tip_amount", "improvement_surcharge", and "total_amount". 
  - We removed rows in "RatecodeID" that are not within the 1 through 6 range. 
  - The remaining outliers are imputed in the data. Although keeping outliers may reduce the statistical significance, they naturally occurred and not caused by statistical errors. Therefore, they were true outliers that may contains important statistical information that worth keeping in the data.
  - See below for the subset of data called "sub_tripdata" with 15 independent variables and 1 dependent variable(tip_amount). 
```{r}
head(sub_tripdata)
```


•  Normalize the data: perform either max-min normalization or z-score standardization on the continuous variables/features. 
```{r}
library(caret)
```

```{r}
#Encode the data
#Converting categorical variables to factor data type. 
#VendorID, RatecodeID, passenger_count, mta_tax, improvement_surcharge, payment_type, trip_type, and extra.
sub_tripdata$VendorID<-as.factor(sub_tripdata$VendorID)
sub_tripdata$RatecodeID<- as.factor(sub_tripdata$RatecodeID)
sub_tripdata$mta_tax<- as.factor(sub_tripdata$mta_tax)
sub_tripdata$improvement_surcharge<- as.factor(sub_tripdata$improvement_surcharge)
sub_tripdata$payment_type<- as.factor(sub_tripdata$payment_type)
sub_tripdata$trip_type<- as.factor(sub_tripdata$trip_type)
sub_tripdata$extra<- as.factor(sub_tripdata$extra)
str(sub_tripdata)
```

```{r}
#normalize all continuous independent variables, but will leave Factor variables as is:
norma<-preProcess(sub_tripdata[,1:14], method = c("range") )
normalized_trip<- predict(norma, sub_tripdata[,1:14])
str(normalized_trip)
```

•  Encode the data: determine if there are any categorical variables that need to be encoded and perform the encoding. 

  - Encoding factor variables into numeric using dummy codes. 
```{r}
normalized_trip<- cbind(normalized_trip, tip_amount=sub_tripdata[,15])
#normalized_trip
```

  - Factor variables with 2 levels: VendorID, mta_tax, improvement_surcharge, and trip_type. 
```{r}
normalized_trip$mta_tax <- ifelse(normalized_trip$mta_tax == "0.5", 1, 0)

normalized_trip$trip_type<- ifelse(normalized_trip$trip_type=="2", 1, 0)
normalized_trip$improvement_surcharge<- ifelse(normalized_trip$improvement_surcharge=="0.3",1, 0)
normalized_trip$VendorID<- ifelse(normalized_trip$VendorID=="2", 1, 0)

```

  - Factor variables with 3 or more levels: RatecodeID, extra, payment_type.
```{r}
RatecodeID<- as.data.frame(dummy.code(normalized_trip$RatecodeID))
payment_type<- as.data.frame(dummy.code(normalized_trip$payment_type))
extra<- as.data.frame(dummy.code(normalized_trip$extra))
```

  - Combine new dummy variables with original data set.
```{r}
data_reg <- normalized_trip %>%
  cbind( RatecodeID=RatecodeID, payment_type=payment_type, extra=extra) %>%
  select(tip_amount, everything(),-c(RatecodeID, payment_type, extra))

head(data_reg)
```

•  Prepare the data for modeling: shuffle the data and split it into training and test sets. The percent 
split between the training and test set is your decision. However, clearly indicate the reason. 

  - The data will be splitted by 80:20 for training and testing, respectively. We choose this ratio because it is the optimal and most commonly used ratio. 
```{r}
library(tidymodels)
```

```{r}
#splitting the data in to train and test data set at a ratio of 80:20.
set.seed(7788)
trip_split<-initial_split(data_reg, prop=0.8, strata = tip_amount)

trip_train<-trip_split %>%
  training()

trip_test<-trip_split%>%
  testing()
```
 
**Question 3 — (30 points)** 

CRISP-DM: Modeling 

•  In this step you will develop the k-nn regression model. Create a function with the following name and arguments: knn.predict(data_train, data_test, k); 

•  data_train represents the observations in the training set, 

•  data_test represents the observations from the test set, and 

•  k is the selected value of k (i.e. the number of neighbors). 

Perform the following logic inside the function: 

•  Implement the k-nn algorithm and use it to predict the tip amount for each observation in the test 
set i.e. data_test. 

•  Note: You are not required to implement the k-nn algorithm from scratch. Therefore, this step 
may only involve providing the training set, the test set, and the value of k to your chosen k-nn 

•  Calculate the mean squared error (MSE) between the predictions from the k-nn model and the actual tip amount in the test set. 

•  The knn.predict() function should return the MSE. 

```{r}
#knn.predict that returns MSE. 
knn.predict<- function(data_train, data_test, k) {
  kmodel<-FNN::knn.reg(train= data_train[,2:28],test= data_test[,2:28], y=data_train[,1], k=k)
  y_pred<- kmodel$pred
  y_test<- data_test[,1]
  mse<- mean((y_test - y_pred)^2)
  return(mse)
}
```

  - We tested multiple k values below using our knn.predict function.
  - We noticed that as k values increases the MSE tend to increase. For example, when k=20, mse=1.909649. When k=50, mse=2.081655. When k=200, mse= 2.314413. When k=915, mse=2.547468. 
  
```{r}
knn.predict(trip_train,trip_test,1) #1.762807
knn.predict(trip_train,trip_test,2) #1.634324
knn.predict(trip_train,trip_test,3) #1.620225
knn.predict(trip_train,trip_test,4)  #1.61445
knn.predict(trip_train,trip_test,5)  #1.642682
knn.predict(trip_train,trip_test,20)  #1.909649

#knn.predict(trip_train,trip_test,50)  #2.081655
#knn.predict(trip_train,trip_test,200)  #2.314413
#knn.predict(trip_train,trip_test,915)   #2.547468
```

  
**Question 4 — (30 points)** 

CRISP-DM: Evaluation 
•  Determine the best value of k and visualize the MSE. This step requires selecting different values 
of k and evaluating which produced the lowest MSE. At a minimum, ensure that you perform the 
following: 

•  Provide at least 20 different values of k to the knn.predict() function (along with the training set 
and the test set). 

  - As we tested our knn.predict function in the previous question, mse tends to increase as k increases. Thus, we decided to choose k values between 1 and 20. 
  - We used two method to test 20 different k-values: for loop and sapply(), which yield the same results. 
```{r}
#using loop
mse_value<-c()
for (k in 1:20){
  kmse<-knn.predict(trip_train,trip_test,k)
  mse_value<- c(mse_value,kmse)
}
mse_value
```

```{r}
# define 20 different values of k to evaluate using sapply
k = c(1:20)

#using sapply
knn_MSE=sapply(k,knn.predict,data_train=trip_train,data_test=trip_test)
knn_MSE
```

  - "knn_result" data frame below displays the k values and associated MSE from lowest to highest. 
```{r}
#knn_results
knn_result<-data.frame(k,MSE=knn_MSE)
knn_result<-knn_result%>%
  arrange(MSE)
knn_result



#best K
best_k=k[which.min(knn_MSE)]
best_k
```

Tip: use a loop! Use a loop to call knn.predict() 20 times and in each iteration of the loop, provide 
a different value of k to knn.predict(). Ensure that you save the MSE that’s returned. 

•  Create a line chart and plot each value of k on the x-axis and the corresponding MSE on the y-axis. 
Explain the chart and determine which value of k is more suitable and why. 

  - The line chart below shows the MSE of each k value from 1 to 20. We can see that MSE is lowest when k=4. 
```{r}
ggplot(knn_result,aes(x=k,y=MSE,label=k)) +
geom_line(color = "green") + geom_point(color = "blue") + geom_text(hjust=0, vjust=-1)+
  ggtitle("Mean Squared Error of Each K value from 1 to 20")
```

•  What are your thoughts on the model that you developed and the accuracy of its predictions? 
Would you advocate for its use to predict the tip amount of future trips? Explain your answer. 
```{r}
# obtaining an error in the first chunk regarding tip amount data frame.
#Creating separate dataframe for 'Tip amount' feature which is our target.

train_tip_amount <-trip_train[,1]
test_tip_amount <- trip_test[,1]
```

```{r}
#model accuracy when k=4
k1 <- FNN::knn.reg(train= trip_train[,2:28],test= trip_test[,2:28], y=train_tip_amount, k=4)
accuracy <- 100 * sum(test_tip_amount == k1$pred)/NROW(test_tip_amount)
accuracy  

k20 <- FNN::knn.reg(train= trip_train[,2:28],test= trip_test[,2:28], y=train_tip_amount, k=20)
k20_accuracy <- 100 * sum(test_tip_amount == k20$pred)/NROW(test_tip_amount)
k20_accuracy
```

  - The model accuracy is about 55% when k is 4. 
 
**Question 5 — (10 optional/bonus points) **

In this optional (bonus) question, you can: 1) use your intuition to create a compelling visualization that 
tells an informative story about one aspect of the dataset OR 2) optimize the k-nn model and evaluate the 
effect of the percentage split, between the training and test set, on the MSE. Choose ONE of the 
following: 

• Create a compelling visualization that tells an informative story about how these cabs are used. 
OR 

• Evaluate the effect of the percentage split for the training and test sets and determine if a different split 
ratio improves your model’s ability to make better predictions. 

Ensure that you perform the steps of the bonus question in a new R chunk! 
 
 
Note: all charts that are displayed should have the following: 

• An informative title (and subtitle if applicable) 

• Labels on the x-axis and y-axis that indicate the units of measurement. 

• A caption that indicates the purpose of the chart. 

```{r}
# train and test data set splited at a ratio of 70:30.
set.seed(1357)

trip_split1<-initial_split(data_reg, prop=0.7, strata = tip_amount)

trip_train1<-trip_split1 %>%
  training()

trip_test1<-trip_split1%>%
  testing()
```


```{r}
#Creating separate dataframe for 'Tip amount' feature which is our target.
train_tip_amount <-trip_train1[,1]
test_tip_amount <- trip_test1[,1]
```


```{r}
#Checking for MSE for k values 1:20
k<- c(1:20)
kdf_one <- data.frame(k)
kdf_one$mse <- 0

for (i in 1:nrow(kdf_one)){
k <- kdf_one$k[i]
kdf_one$mse[i] <- knn.predict(trip_train1,trip_test1,k)
}

kdf_one

#best k 
kdf_one$k[which.min(kdf_one$mse)]
```

  - Best k is 3. 

```{r}
#build knn regression model using best k (3). 
k1_test <-  FNN::knn.reg(train= trip_train1[,2:28],test= trip_test1[,2:28], y=train_tip_amount, k=3)

#Checking for accuracy
accuracy_test <- 100 * sum(test_tip_amount == k1_test$pred)/NROW(test_tip_amount)
accuracy_test
```

  - The accuracy of a 70:30 split model is 57.24%, when k is 3.


```{r}
#kdf_diff <- knn_result[c(1:4,17:20),2]
kdf_one
#kdf_diff
knn_result
kdf_one <- cbind(kdf_one,knn_result)

kdf_one<-kdf_one%>%
full_join(knn_result) %>%
  rename("mse_values_70" = "MSE","mse_values_80" = "mse")

kdf_one

plot <- ggplot(kdf_one, aes(x = k)) + geom_line(aes(y = mse_values_70, colour = mse_values_70),size = 1.2) + geom_line(aes(y = mse_values_80, colour = mse_values_80), size = 1.2) + theme_classic() + labs(title = "K_Values to Mean Square Errors Analysis", x = "Values of K", y = "Mean Squared Errors (MSE)" )

plot
```


